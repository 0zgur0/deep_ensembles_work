# Questions

## How do ensembles differ from single models?

1. **In-distribution accuracy** (ensembles are better; known from boosting/bagging literature)
   - Given an fixed parameter count, ensembles (of smaller models) tend to perform better than one large model
2. **Shifted/out-of-distribution accuracy** (ensembles are better, but they are not "effectively robust". See plot: ![example_model_11-04-21_23_50 25](https://user-images.githubusercontent.com/824157/140568852-d14b6093-59df-4253-bae3-3d7292129192.png)
3. **In-distribution calibration** (ensembles are better, but postprocessing makes single networks competitive)
   - Is there some notion of "effectively well-calibrated," similar to the notion of "effectively robust"?
   - In other words, are ensembles well-calibrated because they fundamentally differ from single models, or are InD calibration and InD accuracy linearly related, like InD/OOD accuracy?
4. **Out-of-distribution calibration** (ensembles are better, see Ovadia et al. 2019)
5. **Feature diversity**
   - See Fort et al., 2019
6. **OOD detection**


## Given that differences exist, can we make a single model behave more like an ensemble, or vice versa?

- Can we make ensembles "more diverse" by introducing some dependency between them?
   - E.g. can we use a diversity regularizer?
   - E.g. what is the ensemble equivalent of quasi-Monte carlo, or a determinental point process?  
- Can we introduce a new loss function that makes single models behave more like ensembles?


## What do we actually know about ensembles, versus what is unfounded intuition?

- TODO: examine foundational ensemble literature (Dietterich, Scaphire, Freund, Brieman, etc.)
- TODO: examine more recent literature (Lakshminarayan et al., 2017; Fort et al., 2019; Ovadia et al., 2019; Havasi et al., 2021)


# Background

## Some interesting papers related to this discussion

- Havasi et al. 2021; "Training independent subnetworks for robust prediction", ICLR 2021.
   - TLDR: a single network is trained to take in multiple inputs and produce predictions for each image. At test time, the authors claim that it functions similarly to an ensemble.
- Miller et al., 2021; "Accuracy on the Line: On the Strong Correlation Between OOD and InD Generalization"
Extends Taori et al. to more datasets and shifts (train on CIFAR-10 or ImageNet, test on synthetic/real world shifts)
   - TLDR: for any pair of InD/OOD test datasets, there appears to be a linear correlation between InD accuracy and OOD accuracy.
   - This paper does not test ensembles, however
- Minderer et al., 2021; "Revisiting the Calibration of Modern Neural Networks"
   - TLDR: this paper claims that bigger models from 1-2 years ago are now well calibrated, though this claim is a bit suspect. 


## Connections to other topics

- Underspecification in neural networks / The "Rashomon Effect" (D'Amour et al. 2020; https://arxiv.org/abs/2011.03395, D'Amour, 2021; https://arxiv.org/pdf/2104.02150.pdf)
- Mixture of experts models
   - MoE models aim to achieve specialization. We only penalize the expert model, not the whole set of models.
   - On the other hand, the loss function when training ensembles penalizes the *average* model performance. In some sense, it seems like ensembles are being trained for redundency, rather than specialization.
   - Do the benefits of ensembles go away when we weight the component models unevenly, or if their weighting is input dependent (as with a MoE)?


# Experiments

1. Interpolation between an ensemble and a big neural network
   - We simultaneously train a big neural network and an ensemble, where both share the same set of parameters. The loss function is a convex combination of the neural network/ensemble losses.
   - As we interpolate between an ensemble and a big model, what changes do we notice
2. New (ensemble) loss for neural networks
   - Divide the features/weights into M groups, \phi_i and w_i
   - Standard network loss: -logsoftmax ( 1/M \sum_{i=1}^M \phi_i^t w_i )
   - Ensemble loss: 1/M \sum_{i=1}^M -logsoftmax( \phi_i^t w_i )
   - Ensemble loss is an upper bound on the standard network loss (Jensen's inequality)
